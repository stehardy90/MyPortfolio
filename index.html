<!DOCTYPE html>
<html>
<head>

    <link rel="icon" type="image/png" href="page_images/favicon.png">

    <style>
        body {
            font-family: Calibri, sans-serif;
            margin: 0;
            padding: 0px;
            background-color: #e9edf0;
            display: flex;
            flex-direction: column;
            min-height: 100vh;
        }
        .content {
            max-width: auto;
            margin: 0px;
            padding-top: 0px;
            padding: 20px;
            background-color: #e9edf0;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            flex: 1;
        }
        .banner-container {
            margin: 0px auto;
            padding: 0;
            text-align: center;
        }
        .sub-banner-container {
            width: calc(100% - 0px);
            max-width: 100%;
            height: auto;
            margin: 0 auto;
            display: block;
        }
        .banner {
            width: 100%;
            height: auto;
            margin: 0;
            padding: 0;
        }
        h1 {
            color: #333;
            text-align: center;
        }
        p {
            line-height: 1.6;
        }
        .scripts-container {
            display: flex;
            gap: 20px;
            justify-content: space-between;
        }
        .script-box {
            flex: 1;
            background-color: #fff;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        }
        .intro-box {
            background-color: #fff;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            margin-bottom: 20px;
        }
        
        .project-box {
            background-color: #fff;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            margin-bottom: 20px;
            width: 100%; /* Make sure the box spans the entire width */
            box-sizing: border-box; /* Ensure padding is included within the width */
            display: block; /* Make sure each box behaves like a block element */
        } 
        
        .tech-stack-container {
            display: flex;
            justify-content: space-between;
            margin: 10px 0;
            gap: 10px;
        }
        
        .tech-box {
            background-color: #fff;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            text-align: center;
            display: flex;
            flex-direction: column;
            justify-content: flex-start;  /* Align grid to the top of the box */
            margin-bottom: 10px;
            width: 100%;
            height: auto;  /* Adjust based on content */
        }
        
        .tech-box h3 {
            margin-top: 5px;  /* Reduce the gap between the title and the top of the box */
        }
        
        .tech-box img {
            max-width: 70%;    /* Reduce the max-width to make the images slightly smaller */
            max-height: 70%;   /* Reduce the max-height as well */
            object-fit: contain; /* Maintain the aspect ratio without stretching */
            display: block;
        }
        
        .tech-images {
            display: grid;
            grid-template-columns: repeat(2, 1fr);  /* 2 columns */
            grid-template-rows: repeat(3, 1fr);     /* 3 rows */
            gap: 10px;
            justify-items: center;   /* Center images horizontally */
            align-items: center;     /* Center images vertically */
            height: 100%;            /* Ensure the grid takes up full height of the box */
            width: 100%;             /* Full width of the box */
        }
        
        #data-engineering-box {
            min-height: 450px;   /* Make it taller to accommodate 5 images */
        }
        
        #data-engineering-box .tech-images img:nth-child(5) {
            grid-column: span 2;  /* Make the 5th image span across two columns, centering it */
        }
        
        .social-links {
            text-align: center;
            padding: 20px;
            background-color: #e9edf0;
            margin-top: auto;
        }
        
        .social-links a {
            margin: 0 15px;
            text-decoration: none;
        }
        
        .social-links img {
            width: 40px;
            height: auto;
        }

        .center-text {
            display: block; /* Ensure it's treated as a block element */
            text-align: center; /* Centers the text */
            margin: 0 auto; /* Optional: Ensures it's centered within its container */
            font-style: italic; /* Keeps the italic style */
        }

        .screenshots-grid {
            display: grid;
            grid-template-columns: repeat(3, 1fr); /* 3 columns */
            gap: 20px; /* Space between the screenshots */
            justify-items: center; /* Center the images horizontally */
            align-items: center; /* Center the images vertically */
            margin-top: 20px;
        }
        
        .screenshots-grid img {
            max-width: 100%; /* Ensure the images fit within their containers */
            height: auto; /* Maintain original aspect ratio */
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); /* Add a subtle shadow for better visuals */
            cursor: pointer; /* Show a pointer when hovering */
        }

        .modal {
            display: none; /* Hidden by default */
            position: fixed; /* Stay in place */
            z-index: 1; /* Sit on top */
            padding-top: 100px; /* Location of the box */
            left: 0;
            top: 0;
            width: 100%; /* Full width */
            height: 100%; /* Full height */
            overflow: auto; /* Enable scroll if needed */
            background-color: rgba(0, 0, 0, 0.9); /* Black background with opacity */
        }
        
        /* Modal Content (the image) */
        .modal-content {
            margin: auto;
            display: block;
            max-width: 80%; /* Set a max width for the image */
            max-height: 80%; /* Set a max height for the image */
        }
        
        /* Close Button (the "X") */
        .close {
            position: absolute;
            top: 50px;
            right: 80px;
            color: white;
            font-size: 40px;
            font-weight: bold;
            cursor: pointer;
        }
        
        /* Add Animation (optional) */
        .modal-content {
            animation-name: zoom;
            animation-duration: 0.6s;
        }
        
        @keyframes zoom {
            from {transform: scale(0)}
            to {transform: scale(1)}
        }

    </style>
</head>
<body>
    <!-- Banner Section -->
    <div class="banner-container">
        <img src="page_images/BANNER.png" alt="Banner Image" class="banner">
    </div>
    
    <!-- Content Section -->
    <div class="content">
        <!-- Introduction Section -->
        <div class="intro-box">
            <p style="margin-top: 0;">Welcome to my portfolio!</p>
        <p>I’m Steven, a data engineer with 10 years of experience specialising in SQL, ETL pipelines, and cloud platforms. Throughout my career, I’ve built and optimised scalable data solutions that drive efficiency and enable data-driven decision-making. Data engineering is more than just a profession for me—it's a passion. I’m dedicated to ensuring that every project I take on delivers meaningful value and empowers organisations to grow through insightful, data-backed strategies.</p>
        <p>To respect the privacy and intellectual property of my current and former employers, this portfolio showcases personal projects I've developed in my own time, demonstrating my skills in SQL, Python, and ETL automation. For a full breakdown of the technologies I’ve applied across both my professional and personal work, be sure to explore the "Technology Stack" section below.</p>
        <p>I also invest heavily in continuous learning, staying current with the latest trends and exploring new technologies to keep pace in this rapidly evolving field.</p>
        <p>Feel free to explore my work and reach out with any questions!</p>
        </div>

        <!-- Batch Data Banner -->

        <div class="sub-banner-container">
            <img src="page_images/BATCH DATA.png" alt="Batch Data Banner" class="banner">
        </div>
        </br>        
            <a href="https://github.com/stehardy90/football-data-pipeline/tree/master" target="_blank">
            <img src="page_images/footballarch_v2.png" alt="Football Pipeline Architecture" style="display: block; margin: 0 auto; max-width: 100%; height: auto;">
            </a>
        <i class="center-text">Click the image above to access the project repository.</i>
        </br>
        </br>
            <div class="project-box">
                <strong>Project Purpose and Overview</strong> 
                </br>
                </br>
                This project demonstrates a complete data engineering pipeline, built using <b>Medallion Architecture</b> principles. Data flows through structured layers—Bronze for raw ingestion, Silver for transformation and validation, and Gold for aggregated insights—ensuring <b>scalability</b>, <b>data lineage</b>, and <b>traceability</b>.
                <p>Football-related data is ingested from the football-data.org API into <b>Google BigQuery</b> using <b>Python</b>. The <b>dbt</b> models are responsible for processing and transforming the raw JSON data through the Silver and Gold layers, while <b>Apache Airflow</b> handles orchestration and scheduling. <b>Docker</b> ensures consistent deployment across environments.</p>
                <p>The project incorporates key aspects of modern data engineering workflows, such as <b>incremental data loading</b>, <b>Type 2 Slowly Changing Dimensions (SCDs)</b>, and <b>real-time error monitoring and alerting</b> via Slack notifications.</p>
                <strong>Purpose</strong>
                <ul>
                    <li><strong>Skill Development:</strong> The primary goal of this project was to deepen my experience with modern data engineering tools, such as dbt, Apache Airflow, Google BigQuery, and Docker. These tools are widely used in industry to build scalable, efficient, and maintainable data pipelines.</li>
                    <li><strong>Learning Opportunity:</strong> Working on this project helped me tackle complex data orchestration, scheduling, and incremental processing concepts. Despite the limitations of the free data plan, the emphasis was on robust pipeline architecture and best practices.</li>
                </ul>
            </div>

            <div class="screenshots-grid">
            <img src="page_images/etlperformance.png" alt="ETL performance dashboard" onclick="openModal('page_images/etlperformance.png')">
            <img src="page_images/slackalerts.png" alt="Slack alerts" onclick="openModal('page_images/slackalerts.png')">
            <img src="path_to_thumbnail1.png" alt="Screenshot 1" onclick="openModal('path_to_full_image1.png')">
            </a>
            </div>

            <div id="lightbox-modal" class="modal">
                <span class="close" onclick="closeModal()">&times;</span>
                <img class="modal-content" id="lightbox-image">
            </div>

        
        </br>

        <!-- Streaming Work in Progress Banner -->
        <div class="sub-banner-container">
            <img src="page_images/STREAMING WORK IN PROGRESS.png" alt="Streaming Work In Progress Banner" class="banner">
        </div>

        </br>

        <!-- WIP Image -->
        
        <img src="page_images/reddit_stream.png" alt="Reddit realtime stream" style="display: block; margin: 0 auto; max-width: 100%; height: auto;">
        </br> 
        <div class="intro-box">
        <p>I’m currently (Oct '24) developing a real-time data streaming pipeline that leverages PRAW, Kafka, Apache Spark and cassandra to stream and analyse Reddit comments. The project aims to provide live insights into trending topics, enhanced by emotion analysis and keyword extraction. The above image details my current progress, stay tuned for further updates as I develop my project!</p>
        </div>
        </br>

        <!-- SQL Scripts Section -->
        <div class="sub-banner-container">
            <img src="page_images/SQL.png" alt="SQL Banner" class="banner">
        </div>     

        </br>

        <div class="scripts-container">
            <div class="script-box">
                <strong>Script Name:</strong> 
                <a href="https://github.com/stehardy90/DataEngineerPortfolio/blob/main/sql_scripts/monthly_revenue_ranking.sql" target="_blank">
                    <em>monthly_revenue_ranking.sql</em>
                </a><br>
                </br>
                <strong>Description:</strong> Calculates total revenue by supplier over the past 12 months. It ranks suppliers by revenue for each period, returning the top 5 and bottom 5 suppliers for each month.<br>
                </br>
                <strong>Key Details:</strong>
                <ul>
                    <li><strong>Purpose:</strong> To calculate total monthly gross revenue for each supplier, returning the top 5 and bottom 5 performers in each period.</li>
                    <li><strong>Performance:</strong> The script runs in approximately 1 second on a dataset with over 10,000,000 rows in the orders table.</li>
                    <li><strong>Database:</strong> Designed for use on a SQL Server database.</li>
                    <li><strong>Key Competencies:</strong> Advanced data aggregation, performance optimisation, window functions and ranking, dynamic date filtering, common table expressions (CTEs)</li>
                </ul>
            </div>

            <div class="script-box">
                <strong>Script Name:</strong> 
                <a href="https://github.com/stehardy90/DataEngineerPortfolio/blob/main/sql_scripts/etl_UpdateFactOrders.sql" target="_blank">
                    <em>etl_UpdateFactOrders.sql</em>
                </a><br>
                </br>
                <strong>Description:</strong> An ETL stored procedure to update the fact.orders table by processing order data from the staging area, applying performance optimisations, setting specific flags (e.g., non-delivery), and merging updates into the fact table.<br>
                </br>
                <strong>Key Details:</strong>
                <ul>
                    <li><strong>Purpose:</strong> The procedure ensures that the fact.orders table is up-to-date with the latest order data, using efficient filtering and caching mechanisms to handle only relevant records, improving performance and data accuracy.</li>
                    <li><strong>Performance:</strong> The script runs in < 1 minute.</li>
                    <li><strong>Database:</strong> Designed for use on a SQL Server database.</li>
                    <li><strong>Key Competencies:</strong> SQL ETL processing, error handling, merge logic, performance monitoring</li>
                </ul>
            </div>

            <div class="script-box">
                <strong>Script Name:</strong> 
                <a href="https://github.com/stehardy90/DataEngineerPortfolio/blob/main/sql_scripts/snowflake_salary_masking.sql" target="_blank">
                    <em>snowflake_salary_masking.sql</em>
                </a><br>
                <strong>Description:</strong> Creates a dynamic masking policy named salary_masking_policy for the salary field, which reveals the actual salary value only to users with the LEAD_DEVELOPER role and masks it for all other users. It then applies this masking policy to the salary column in the employee table.<br>
                </br>
                <strong>Script Name:</strong> 
                <a href="https://github.com/stehardy90/DataEngineerPortfolio/blob/main/sql_scripts/snowflake_dynamic_pivot.sql" target="_blank">
                    <em>snowflake_dynamic_pivot.sql</em>
                </a><br>
                <strong>Description:</strong> Dynamically pivots the results of the 'SupplierSales' CTE, making use of the Snowflake 'ANY' keyword. Without 'ANY', each SUPPLIER_ID would need to be explicitly specified, which is inefficient and difficult to maintain when IDs frequently change.<br>
                </br>
                <strong>Script Name:</strong> 
                <a href="https://github.com/stehardy90/DataEngineerPortfolio/blob/main/sql_scripts/snowflake_timetravel.sql" target="_blank">
                    <em>snowflake_timetravel.sql</em>
                </a><br>
                <strong>Description:</strong> Uses Snowflakes timetravel functionality to access and query historical data from a previous point in time, useful for recovering accidentally deleted or modified data, auditing changes, and comparing past and current states of data without needing backups or manual version control.<br>
            </div>
        </div>

        </br>

        <!-- Python Scripts Section -->
        <div class="sub-banner-container">
            <img src="page_images/PYTHON SCRIPTS.png" alt="Python Scripts Banner" class="banner">
        </div>     

        </br>

        <!-- Placeholder Section for Python Scripts -->
        <div class="scripts-container">
            <div class="script-box">
                <strong>Script Name:</strong> 
                <a href="https://github.com/stehardy90/DataEngineerPortfolio/blob/main/python_scripts/reddit_kafka_streamer.py" target="_blank">
                    <em>reddit_kafka_streamer.py</em>
                </a><br><br>
                <strong>Description:</strong> This script streams real-time comments from a specified subreddit using the Reddit API (PRAW) and sends the structured comment data to a Kafka topic. It integrates environment variables for Reddit API credentials and the subreddit name using python-dotenv for flexibility. Additionally, the script implements retry logic for establishing a Kafka producer connection, ensuring reliable data ingestion into Kafka.<br>
                <br>
                <strong>Key Competencies:</strong>
                <ul>
                    <li><strong>Kafka Integration:</strong> Streams Reddit comments to Kafka by producing structured comment data to the 'reddit-comments' topic for further downstream processing.</li>
                    <li><strong>Reddit API (PRAW):</strong> Utilises PRAW to interact with the Reddit API, streaming live comments from the 'news' subreddit in real-time.</li>
                    <li><strong>Resilient Kafka Producer:</strong> Implements retry logic for Kafka producer connection, ensuring robustness by handling connection failures with exponential backoff.</li>
                    <li><strong>Environment Variable Management:</strong> Loads sensitive configuration details, such as Reddit API credentials, from environment variables using the dotenv package for secure configuration.</li>
                    <li><strong>Data Structuring:</strong> Organises Reddit comment data and parent post information into a structured JSON format, making it suitable for streaming and further processing in downstream systems.</li>
                    <li><strong>Real-Time Data Streaming:</strong> Streams live data from Reddit into Kafka, providing a real-time feed of user-generated content for analysis or further processing.</li>
                    <li><strong>Error Handling:</strong> Manages Kafka connection failures with a retry mechanism, ensuring resilience and reliability in a production environment.</li>
                </ul>
            </div>

            <div class="script-box">
                <strong>Script Name:</strong> 
                <a href="https://github.com/stehardy90/DataEngineerPortfolio/blob/main/python_scripts/football_dbt_etl_pipeline.py" target="_blank">
                    <em>football_dbt_etl_pipeline.py</em>
                </a><br><br>
                <strong>Description:</strong> This Airflow DAG orchestrates an ETL pipeline for football data, leveraging dbt to run data transformations and tests. It ingests competition data, runs dbt models for both silver and gold layers, and executes data quality checks using dbt tests. The pipeline logs detailed ETL metadata to Google BigQuery for auditing, and integrates with Slack to send alerts in case of task failures. The script implements robust error handling and logging, ensuring reliable execution and monitoring of data workflows.<br>
                <br>
                <strong>Key Competencies:</strong>
                <ul>
                    <li><strong>Apache Airflow:</strong> Orchestrates multiple ETL and transformation tasks with well-defined dependencies, using both Bash and Python operators for flexibility.</li>
                    <li><strong>ETL Logging and Auditing:</strong> Logs the ETL run details, including task status, row count, and test results, into Google BigQuery for auditing and monitoring purposes.</li>
                    <li><strong>Slack Alerts:</strong> Automatically triggers Slack notifications upon task failure, providing real-time feedback and error details to the team.</li>
                    <li><strong>Google BigQuery Integration:</strong> Inserts ETL log entries into a BigQuery table, ensuring that each task’s run data is stored for further analysis and reporting.</li>
                    <li><strong>DBT Integration:</strong> Runs and tests dbt models within the pipeline, capturing results and logging test summaries to BigQuery for comprehensive auditing.</li>
                    <li><strong>Custom Python Functions:</strong> Utilizes Python operators to execute dbt tests, capture their output, and analyse test results for pass/fail criteria, enhancing the automation of data quality checks.</li>
                    <li><strong>Subprocess Management:</strong> Uses Python’s subprocess module to run dbt commands and handle the stdout and stderr for detailed logging in Airflow.</li>
                    <li><strong>Flexible Task Management:</strong> Incorporates success and failure callbacks to log results or trigger alerts based on task outcomes, ensuring robust error handling.</li>
                    <li><strong>Data Pipeline Monitoring:</strong> Tracks and logs each step of the ETL process, providing a detailed audit trail of task execution and test performance.</li>
                </ul>
            </div>

            <div class="script-box">
                <strong>Script Name:</strong> 
                <a href="https://github.com/stehardy90/DataEngineerPortfolio/blob/main/python_scripts/reddit_stream_processor.py" target="_blank">
                    <em>reddit_stream_processor.py</em>
                </a><br><br>
                <strong>Description:</strong> Processes live Reddit comments in near real-time using Apache Spark. It streams data from Kafka, applies Named Entity Recognition (NER) with spaCy to extract key entities like people, organisations, and locations, and uses Hugging Face’s DistilRoBERTa model for emotion classification. The processed data, including extracted entities and emotional sentiment, is then stored in Cassandra for scalable analysis. This script demonstrates an efficient pipeline for real-time data processing, NLP, and distributed storage.<br>
                <br>
                <strong>Key Competencies:</strong>
                <ul>
                    <li><strong>Apache Spark:</strong> Powers both real-time data ingestion and transformations, ensuring efficient and scalable processing of Reddit comment streams.</li>
                    <li><strong>Real-Time Data Processing:</strong> Handles live data streams from Kafka in near real-time, allowing for immediate processing and insights.</li>
                    <li><strong>Data Transformations:</strong> Utilises Spark to apply transformations such as entity extraction and emotion analysis, enriching the raw data before storage.</li>
                    <li><strong>Natural Language Processing (NLP):</strong> Integrates spaCy for Named Entity Recognition (NER) and Hugging Face's DistilRoBERTa for emotion classification, adding deeper context and sentiment to the comments.</li>
                    <li><strong>Data Integration and Storage:</strong> Writes processed data to Cassandra, taking advantage of its distributed design for scalable and reliable storage.</li>
                    <li><strong>Apache Kafka Integration:</strong> Streams live Reddit comments through Kafka, ensuring efficient message handling and smooth data flow into the pipeline.</li>
                    <li><strong>Custom UDFs in Spark:</strong> Uses user-defined functions to perform specific tasks, including UUID generation, entity extraction, and emotion classification, within the Spark framework.</li>
                    <li><strong>End-to-End Pipeline:</strong> Combines Kafka, Spark, and Cassandra into a robust pipeline that efficiently processes, transforms, and stores data from Reddit comments.</li>
                    <li><strong>Scalable and Fault-Tolerant:</strong> Designed to scale seamlessly with increasing data volume, ensuring stability and resilience through Spark and Cassandra.</li>
                </ul>
            </div>
        </div>

        </br>

        <!-- Technology Stack Section -->
        <div class="sub-banner-container">
            <img src="page_images/TECHNOLOGY_STACK.png" alt="Technology Stack Banner" class="banner">
        </div>

        <!-- Technology Stack Boxes -->
        <div class="tech-stack-container">
            <!--  <div id="data-engineering-box" class="tech-box"> -->
             <div class="tech-box">
                <h3>Data Engineering & Integration</h3>
                <div class="tech-images">
                    <img src="page_images/airflow_v2.png" alt="Apache Airflow">
                    <img src="page_images/matillion.png" alt="Matillion">
                    <img src="page_images/kafka.png" alt="Kafka">
                    <img src="page_images/ssis.png" alt="SSIS">
                    <img src="page_images/dbt.png" alt="dbt">                    
                </div>
            </div>
            <div class="tech-box">
                <h3>Data Warehousing & Databases</h3>
                <div class="tech-images">
                    <img src="page_images/snowflake.png" alt="Snowflake">
                    <img src="page_images/google_bigquery.png" alt="Google BigQuery">
                    <img src="page_images/ssms.png" alt="SSMS">
                    <img src="page_images/mongo.png" alt="Mongo DB">
                    <img src="page_images/cassandra.png" alt="cassandra">
                </div>
            </div>
            <div class="tech-box">
                <h3>Data Visualisation & Reporting</h3>
                <div class="tech-images">
                    <img src="page_images/PBI_v2.png" alt="Power BI">
                    <img src="page_images/looker.png" alt="Looker">
                    <img src="page_images/ssrs.png" alt="SSRS">
                    <img src="page_images/tableau.png" alt="Tableau.png">
                </div>
            </div>
            <div class="tech-box">
                <h3>Development & Automation</h3>
                <div class="tech-images">
                    <img src="page_images/azure.png" alt="Azure">                    
                    <img src="page_images/docker_v2.png" alt="Docker">
                    <img src="page_images/postman.png" alt="Postman">
                    <img src="page_images/jira.png" alt="Jira">
                    <img src="page_images/miro.png" alt="Miro">
                </div>
            </div>
        </div>

        </br>

                <!-- Other Section -->
        <div class="sub-banner-container">
            <img src="page_images/OTHER_V2.png" alt="Other Banner" class="banner">
        </div>  
    </br>
        <div class="intro-box">
        <p>Thank you for visiting my portfolio! I'm always open to feedback, suggestions, or any inquiries you may have. Feel free to connect with me on LinkedIn or send me an email using the icons below. I'd love to hear from you!</p>
        </div>
        
    </div>

    <!-- Social Links Section -->
    <div class="social-links">
        <!-- LinkedIn Icon -->
        <a href="https://www.linkedin.com/in/steven-hardy-307358147/" target="_blank" title="LinkedIn Profile">
            <img src="https://cdn-icons-png.flaticon.com/512/145/145807.png" alt="LinkedIn Icon">
        </a>
        <!-- Email Icon -->
        <a href="mailto:stehardy90@gmail.com" title="Email Me">
            <img src="https://cdn-icons-png.flaticon.com/512/726/726623.png" alt="Email Icon">
        </a>
    </div>
</body>
</html>
